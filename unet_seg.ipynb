{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ82esH_1lEq"
      },
      "source": [
        "## Project : Cell image segmentation projects\n",
        "\n",
        "Contact: Elena Casiraghi (University Milano elena.casiraghi@unimi.it)\n",
        "\n",
        "Cell segmentation is usually the first step for downstream single-cell analysis in microscopy image-based biology and biomedical research. Deep learning has been widely used for cell-image segmentation.\n",
        "The CellSeg competition aims to benchmark cell segmentation methods that could be applied to various microscopy images across multiple imaging platforms and tissue types for cell Segmentation. The  Dataset challenge organizers provide both labeled images and unlabeled ones.\n",
        "The “2018 Data Science Bowl” Kaggle competition provides cell images and their masks for training cell/nuclei segmentation models.\n",
        "\n",
        "In 2022 another [Cell Segmentation challenge was proposed at Neurips](https://neurips22-cellseg.grand-challenge.org/).\n",
        "For interested readers, the competition proceeding has been published on [PMLR](https://proceedings.mlr.press/v212/)\n",
        "\n",
        "### Project Description\n",
        "\n",
        "In the field of (bio-medical) image processing, segmentation of images is typically performed via U-Nets [1,2].\n",
        "\n",
        "A U-Net consists of an encoder - a series of convolution and pooling layers which reduce the spatial resolution of the input, followed by a decoder - a series of transposed convolution and upsampling layers which increase the spatial resolution of the input. The encoder and decoder are connected by a bottleneck layer which is responsible for reducing the number of channels in the input.\n",
        "The key innovation of U-Net is the addition of skip connections that connect the contracting path to the corresponding layers in the expanding path, allowing the network to recover fine-grained details lost during downsampling.\n",
        "\n",
        "<img src='https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png' width=\"400\"/>\n",
        "\n",
        "\n",
        "At this [link](https://rpubs.com/eR_ic/unet), you find an R implementation of basic U-Nets. At this [link](https://github.com/zhixuhao/unet), you find a Keras implementation of UNets.  \n",
        "Other implementations of more advanced UNets are also made available in [2] at these links: [UNet++](https://github.com/MrGiovanni/UNetPlusPlus)\n",
        "and by the CellSeg organizers as baseline models: [https://neurips22-cellseg.grand-challenge.org/baseline-and-tutorial/](https://neurips22-cellseg.grand-challenge.org/baseline-and-tutorial/)\n",
        "\n",
        "\n",
        "### Project aim\n",
        "\n",
        "The aim of the project is to download the *gray-level* (.tiff or .tif files) cell images from the [CellSeg](https://neurips22-cellseg.grand-challenge.org/dataset/) competition and assess the performance of an UNet or any other Deep model for cell segmentation.\n",
        "We suggest using gray-level images to obtain a model that is better specified on a sub class of images.\n",
        "\n",
        "Students are not restricted to use UNets but may other model is wellcome; e.g., even transformer based model in the [leaderboard](https://neurips22-cellseg.grand-challenge.org/evaluation/testing/leaderboard/) may be tested.\n",
        "Students are free to choose any model, as long as they are able to explain their rationale, architecture, strengths and weaknesses.\n",
        "\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "[1] Ronneberger, O., Fischer, P., Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab, N., Hornegger, J., Wells, W., Frangi, A. (eds) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science(), vol 9351. Springer, Cham. https://doi.org/10.1007/978-3-319-24574-4_28\n",
        "\n",
        "[2] Long, F. Microscopy cell nuclei segmentation with enhanced U-Net. BMC Bioinformatics 21, 8 (2020). https://doi.org/10.1186/s12859-019-3332-1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rQQQ0U9AEFS"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5co9oUAYAEhl",
        "outputId": "50e0723a-f3bf-49dd-a030-b76b5422486c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: torch in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (2.7.1)\n",
            "Requirement already satisfied: filelock in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: setuptools in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (0.22.1)\n",
            "Requirement already satisfied: numpy in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.7.1 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torchvision) (2.7.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (4.13.2)\n",
            "Requirement already satisfied: setuptools in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (2025.5.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from torch==2.7.1->torchvision) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from jinja2->torch==2.7.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /home/agrodowski/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from opencv-contrib-python) (1.26.4)\n",
            "Collecting torchsummary\n",
            "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
            "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
            "Installing collected packages: torchsummary\n",
            "Successfully installed torchsummary-1.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gdown\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install opencv-contrib-python\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oILKAGLVahzA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import tempfile\n",
        "from typing import Callable, List, Tuple\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Cty3s4rIADSz"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = 'data_train/Training-labeled/images/'\n",
        "TRAIN_LABELS_PATH = 'data_train/Training-labeled/labels/'\n",
        "\n",
        "TEST_PATH = 'data_test/Testing/Public/images/'\n",
        "TEST_LABELS_PATH = 'data_test/Testing/Public/labels/'\n",
        "\n",
        "VAL_PATH = 'data_val/Tuning/images/'\n",
        "VAL_LABELS_PATH = 'data_val/Tuning/labels/'\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "  device = torch.device(\"cuda\")\n",
        "  dataloader_kwargs = {\"batch_size\": 32, \"shuffle\": True, \"pin_memory\": True}\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  dataloader_kwargs = {\"batch_size\": 64}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-UF_9PTAjHq"
      },
      "source": [
        "### Data preparation\n",
        "[Browse the data](https://drive.google.com/drive/folders/1MaJibsHYitCPOltxVzYjr3rm5s9Vpjpv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdHuhOqEAlle",
        "outputId": "99f67f73-2de3-4ab6-e74c-53e6eeb21f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0 2793M    0 7354k    0     0  1900k      0  0:25:05  0:00:03  0:25:02 1899kArchive:  data_test.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of data_test.zip or\n",
            "        data_test.zip.zip, and cannot find data_test.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!curl -o data_test.zip \"https://zenodo.org/records/10719375/files/Testing.zip?download=1\"\n",
        "!unzip -d data_test data_test.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx_OhfAwCoG-",
        "outputId": "95e390bc-816b-4982-a56e-6321d5af7ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: https://zenodo.org/records/10719375/files/Training-labeled.zip?download=1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open data_train.zip, data_train.zip.zip or data_train.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!curl -o data_train.zip \"https://zenodo.org/records/10719375/files/Training-labeled.zip?download=1\"\n",
        "!unzip -d data_train data_train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P-POZuxCo9w",
        "outputId": "6a90328e-9894-483d-9edd-8e78b4a826bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: https://zenodo.org/records/10719375/files/Tuning.zip?download=1\n",
            "unzip:  cannot find or open data_val.zip, data_val.zip.zip or data_val.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!curl -o data_val.zip \"https://zenodo.org/records/10719375/files/Tuning.zip?download=1\"\n",
        "!unzip -d data_val data_val.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "zHtJrfY_PFRH"
      },
      "outputs": [],
      "source": [
        "# Partially adapted from https://colab.research.google.com/github/mim-ml-teaching/public-dnn-2024-25/blob/master/docs/DNN-Lab-7-UNet-in-Pytorch-student-version.ipynb\n",
        "class ImageTiffDataset(torch.utils.data.Dataset):\n",
        " def __init__(self,\n",
        "              image_dir: str,\n",
        "              target_dir: str,\n",
        "              cache_dir: str,\n",
        "              filenames: List[str],\n",
        "              transform: torch.nn.Module = transforms.ToTensor(),\n",
        "              target_transform: torch.nn.Module = transforms.ToTensor()):\n",
        "   self.image_dir = image_dir\n",
        "   self.target_dir = target_dir\n",
        "   self.cache_dir = cache_dir\n",
        "   self.filenames = filenames\n",
        "   self.transform = transform\n",
        "   self.target_transform = target_transform\n",
        "\n",
        "   if not os.path.exists(self.cache_dir):\n",
        "     os.mkdir(self.cache_dir)\n",
        "     os.mkdir(os.path.join(self.cache_dir, \"images\"))\n",
        "     os.mkdir(os.path.join(self.cache_dir, \"target\"))\n",
        "\n",
        " def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "   img_filename = self.filenames[idx]\n",
        "    # Change. I had this error: No such file or directory: 'data_train/Training-labeled/labels/cell_00490.tif.tiff'\n",
        "   target_filename = os.path.basename(img_filename) + '.tiff'\n",
        "   target_filename = os.path.basename(img_filename)\n",
        "    # End of change\n",
        "   img_path = os.path.join(self.image_dir, img_filename)\n",
        "   target_path = os.path.join(self.target_dir, target_filename)\n",
        "   img_cache = os.path.join(self.cache_dir, img_filename)\n",
        "   target_cache = os.path.join(self.cache_dir, target_filename)\n",
        "\n",
        "   if not os.path.exists(img_cache):\n",
        "      # Change. I had this error: AttributeError: module 'PIL.Image' has no attribute 'load'\n",
        "     img = Image.load(img_path)\n",
        "      #End of change\n",
        "     img = Image.open(img_path)\n",
        "     img = self.transform(img)\n",
        "     torch.save(img, img_cache)\n",
        "   else:\n",
        "     img = torch.load(img_cache)\n",
        "\n",
        "   if not os.path.exists(target_cache):\n",
        "      # Change. I had this error: AttributeError: module 'PIL.Image' has no attribute 'load'\n",
        "     target = Image.load(target_path)\n",
        "     target = Image.open(target_path)\n",
        "     # End of change\n",
        "     target = self.target_transform(target)\n",
        "     torch.save(target, target_cache)\n",
        "   else:\n",
        "     target = torch.load(target_cache)\n",
        "\n",
        "   return img, target\n",
        "\n",
        " def __len__(self) -> int:\n",
        "   return len(self.filenames)\n",
        "\n",
        "def make_tiff_dataset(image_dir: str, target_dir: str, cache_dir: str):\n",
        " filenames = []\n",
        " with os.scandir(image_dir) as it:\n",
        "   for entry in it:\n",
        "     if not entry.is_file():\n",
        "       continue\n",
        "     lower_name = entry.name.lower()\n",
        "     if lower_name.endswith('.tiff') or lower_name.endswith('.tif'):\n",
        "       filenames.append(entry.name)\n",
        " return ImageTiffDataset(image_dir, target_dir, cache_dir, filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EN25LfWqtzB",
        "outputId": "5eeedf4a-b08a-4223-ebe8-df78295309de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 491\n",
            "Test: 30\n",
            "Val: 58\n"
          ]
        }
      ],
      "source": [
        "train_dataset = make_tiff_dataset(TRAIN_PATH, TRAIN_LABELS_PATH, tempfile.mkdtemp())\n",
        "test_dataset = make_tiff_dataset(TEST_PATH, TEST_LABELS_PATH, tempfile.mkdtemp())\n",
        "val_dataset = make_tiff_dataset(VAL_PATH, VAL_LABELS_PATH, tempfile.mkdtemp())\n",
        "dataloader_train = torch.utils.data.DataLoader(train_dataset, **dataloader_kwargs)\n",
        "dataloader_test = torch.utils.data.DataLoader(test_dataset, **dataloader_kwargs)\n",
        "dataloader_val = torch.utils.data.DataLoader(val_dataset, **dataloader_kwargs)\n",
        "\n",
        "print('Train:', len(train_dataset))\n",
        "print('Test:', len(test_dataset))\n",
        "print('Val:', len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "I2CL1U3VAcUa"
      },
      "outputs": [],
      "source": [
        "def train_unet(\n",
        "   model: torch.nn.Module,\n",
        "   device: torch.device,\n",
        "   train_loader: torch.utils.data.DataLoader,\n",
        "   optimizer: torch.optim.Optimizer,\n",
        "   epoch: int,\n",
        "   log_interval: int):\n",
        " model.train()\n",
        " correct = 0\n",
        " for batch_idx, (data, target) in enumerate(train_loader):\n",
        "   data, target = data.to(device), target.to(device)\n",
        "   optimizer.zero_grad()\n",
        "   output = model(data)\n",
        "   log_probs = F.log_softmax(output, dim=1)\n",
        "   loss = F.nll_loss(log_probs, target)\n",
        "   pred = log_probs.argmax(\n",
        "       dim=1, keepdim=True\n",
        "   )  # get the index of the max log-probability\n",
        "   correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "   if batch_idx % log_interval == 0:\n",
        "     _, _, image_width, image_height = data.size()\n",
        "     print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "         epoch,\n",
        "         batch_idx * len(data),\n",
        "         len(train_loader.dataset),\n",
        "         100.0 * batch_idx / len(train_loader),\n",
        "         loss.item(),\n",
        "       ))\n",
        " print(\n",
        "   \"Train accuracy: {}/{} ({:.0f}%)\".format(\n",
        "       correct,\n",
        "         (len(train_loader.dataset) * image_width * image_height),\n",
        "         100.0 * correct / (len(train_loader.dataset) * image_width * image_height),\n",
        "     )\n",
        " )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypzsKKSK-sMH"
      },
      "source": [
        "## Basic U-Nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "lrGoupPPhOg6"
      },
      "outputs": [],
      "source": [
        "class UNetConvBlock(nn.Module):\n",
        " def __init__(self, in_channels, out_channels):\n",
        "   self.layer = nn.Sequential(\n",
        "       nn.Conv2d(\n",
        "           in_channels,\n",
        "           out_channels,\n",
        "           kernel_size=3,\n",
        "           padding=1,\n",
        "           dilation=0,\n",
        "           padding_mode='reflect'),\n",
        "       nn.ReLU() # Leaky ReLU?\n",
        "   )\n",
        "\n",
        " def forward(self, x):\n",
        "   return self.layer(x)\n",
        "\n",
        "class UNetEncoderBlock(nn.Module):\n",
        " def __init__(self, in_channels: int, out_channels: int, maxpool: bool = True):\n",
        "   assert out_channels > in_channels\n",
        "   if maxpool:\n",
        "     self.layer = nn.Sequential(\n",
        "         UNetConvBlock(in_channels, out_channels),\n",
        "         UNetConvBlock(out_channels, out_channels),\n",
        "         nn.MaxPool2d(2, dilation=0)\n",
        "     )\n",
        "   else:\n",
        "     self.layer = nn.Sequential(\n",
        "         UNetConvBlock(in_channels, out_channels),\n",
        "         UNetConvBlock(out_channels, out_channels)\n",
        "     )\n",
        "\n",
        " def forward(self, x):\n",
        "   return self.layer(x)\n",
        "\n",
        "class UNetDecoderBlock(nn.Module):\n",
        " def __init__(self, in_channels: int, out_channels: int, unmaxpool: bool = True):\n",
        "   assert out_channels < in_channels\n",
        "\n",
        "   if unmaxpool:\n",
        "     assert False\n",
        "     self.layer = nn.Sequential(\n",
        "          # TODO\n",
        "     )\n",
        "   else:\n",
        "     self.layer = nn.Sequential(\n",
        "         UNetConvBlock(in_channels, out_channels),\n",
        "         UNetConvBlock(out_channels, out_channels)\n",
        "     )\n",
        "\n",
        "class UNet(nn.Module):\n",
        " def __init__(self, encoder_channels: List[int], decoder_channels: List[int]):\n",
        "   assert len(encoder_channels) > 0\n",
        "   self.encoder = nn.ModuleList()\n",
        "   self.decoder = nn.ModuleList()\n",
        "\n",
        "    #Expected:\n",
        "    #UNetEncoderBlock(64, 128),\n",
        "    #UNetEncoderBlock(128, 256),\n",
        "    #UNetEncoderBlock(256, 512),\n",
        "    #UNetEncoderBlock(512, 1024, False)\n",
        "   in_channels = 1\n",
        "   for out_channels in encoder_channels[:-1]:\n",
        "     self.encoder.append(UNetEncoderBlock(in_channels, out_channels))\n",
        "     in_channels = out_channels\n",
        "   self.encoder.append(UNetEncoderBlock(in_channels, encoder_channels[-1], maxpool=False))\n",
        "\n",
        "   for out_channels in decoder_channels[:-1]:\n",
        "     self.decoder.append(in_channels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "STY8vVG5-FtN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def find_image_mask_pairs(images_dir, masks_dir):\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.endswith('.tiff') or f.endswith('.tif')]\n",
        "    mask_files = set(f for f in os.listdir(masks_dir) if f.endswith('.tiff') or f.endswith('.tif'))\n",
        "\n",
        "    pairs = []\n",
        "    for img_file in image_files:\n",
        "        if img_file.endswith('.tiff'):\n",
        "            base_name = img_file[:-5]\n",
        "        else:\n",
        "            base_name = img_file[:-4]\n",
        "\n",
        "        candidate_mask_tiff = base_name + '_label.tiff'\n",
        "        candidate_mask_tif = base_name + '_label.tif'\n",
        "\n",
        "        if candidate_mask_tiff in mask_files:\n",
        "            pairs.append((img_file, candidate_mask_tiff))\n",
        "        elif candidate_mask_tif in mask_files:\n",
        "            pairs.append((img_file, candidate_mask_tif))\n",
        "        else:\n",
        "            print(f\"Brak maski dla obrazu {img_file}\")\n",
        "\n",
        "    return pairs\n",
        "\n",
        "class CellSegDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, target_size=(128,128)):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.target_size = target_size\n",
        "        self.pairs = find_image_mask_pairs(images_dir, masks_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, mask_name = self.pairs[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
        "\n",
        "        # Wczytanie obrazów w trybie grayscale\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is None:\n",
        "            return None, None\n",
        "        if mask is None:\n",
        "            return None, None\n",
        "        \n",
        "        # Zmiana rozmiaru\n",
        "        image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_AREA)\n",
        "        mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Normalizacja obrazu do zakresu [0, 1]\n",
        "        image = image.astype('float32') / 255.0\n",
        "        # Maskę możesz zostawić w oryginalnej skali (0/255), albo też znormalizować\n",
        "        mask = mask.astype('float32') / 255.0\n",
        "\n",
        "        # Dodaj kanał (C=1) dla kompatybilności z siecią (C,H,W)\n",
        "        image = image[None, :, :]\n",
        "        mask = mask[None, :, :]\n",
        "\n",
        "        # Zamiana na tensory torch\n",
        "        image = torch.from_numpy(image)\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "YtJfcBQd6O66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TIFF File Diagnosis\n",
            "============================================================\n",
            "\n",
            "File 1: cell_00279.tif\n",
            "  OpenCV: shape=(1608, 1608), dtype=uint16, range=[100, 276]\n",
            "  PIL: mode=I;16, size=(1608, 1608), array_shape=(1608, 1608), dtype=uint16\n",
            "\n",
            "File 2: cell_00379.tif\n",
            "  OpenCV: shape=(970, 970), dtype=uint8, range=[6, 245]\n",
            "  PIL: mode=L, size=(970, 970), array_shape=(970, 970), dtype=uint8\n",
            "\n",
            "File 3: cell_00644.tif\n",
            "  OpenCV: shape=(420, 350), dtype=uint16, range=[328, 6859]\n",
            "  PIL: mode=I;16, size=(350, 420), array_shape=(420, 350), dtype=uint16\n",
            "\n",
            "File 4: cell_00352.tif\n",
            "  OpenCV: shape=(970, 970), dtype=uint8, range=[4, 255]\n",
            "  PIL: mode=L, size=(970, 970), array_shape=(970, 970), dtype=uint8\n",
            "\n",
            "File 5: cell_00263.tif\n",
            "  OpenCV: shape=(1608, 1608), dtype=uint16, range=[99, 309]\n",
            "  PIL: mode=I;16, size=(1608, 1608), array_shape=(1608, 1608), dtype=uint16\n",
            "\n",
            "Testing dataset with OpenCV...\n",
            "Found 491 TIFF image-mask pairs\n",
            "Using OpenCV for image loading\n",
            "\n",
            "[First TIFF Image Info]\n",
            "Image path: data_train/Training-labeled/images/cell_00279.tif\n",
            "Original shape: (1608, 1608), dtype: uint16\n",
            "After resize: (512, 512), dtype: float32\n",
            "Value range: [101.86932373046875, 253.8798828125]\n",
            "Mask unique values: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109.]\n",
            "Sample 0: Image torch.Size([1, 512, 512]), Mask torch.Size([512, 512]) ✓\n",
            "Sample 1: Image torch.Size([1, 512, 512]), Mask torch.Size([512, 512]) ✓\n",
            "Sample 2: Image torch.Size([1, 512, 512]), Mask torch.Size([512, 512]) ✓\n"
          ]
        }
      ],
      "source": [
        "## FIXED DATASET ????\n",
        "\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image  # Alternative for handling complex TIFFs\n",
        "\n",
        "class CellSegmentationDatasetCV2(Dataset):\n",
        "    \"\"\"Robust dataset for TIFF files with better error handling\"\"\"\n",
        "    \n",
        "    def __init__(self, image_dir, mask_dir, target_size=(512, 512), use_pil=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.target_size = target_size\n",
        "        self.use_pil = use_pil  # Option to use PIL instead of OpenCV\n",
        "        \n",
        "        # Only process TIFF files\n",
        "        valid_exts = ('.tif', '.tiff')\n",
        "        \n",
        "        # Collect valid image-mask pairs\n",
        "        self.image_mask_pairs = []\n",
        "        for fname in os.listdir(image_dir):\n",
        "            if fname.lower().endswith(valid_exts):\n",
        "                name, _ = os.path.splitext(fname)\n",
        "                # Look for corresponding mask\n",
        "                for ext in valid_exts:\n",
        "                    mask_fname = f\"{name}_label{ext}\"\n",
        "                    mask_path = os.path.join(mask_dir, mask_fname)\n",
        "                    if os.path.isfile(mask_path):\n",
        "                        self.image_mask_pairs.append((fname, mask_fname))\n",
        "                        break\n",
        "        \n",
        "        if len(self.image_mask_pairs) == 0:\n",
        "            raise ValueError(f\"No valid TIFF image-mask pairs found\")\n",
        "        \n",
        "        print(f\"Found {len(self.image_mask_pairs)} TIFF image-mask pairs\")\n",
        "        print(f\"Using {'PIL' if use_pil else 'OpenCV'} for image loading\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_mask_pairs)\n",
        "    \n",
        "    def _load_and_resize_with_pil(self, path, is_mask=False):\n",
        "        \"\"\"Load and resize using PIL (more robust for complex TIFFs)\"\"\"\n",
        "        img = Image.open(path)\n",
        "        \n",
        "        # Convert to numpy array\n",
        "        img_array = np.array(img)\n",
        "        \n",
        "        # Resize using PIL\n",
        "        if is_mask:\n",
        "            img = img.resize(self.target_size, Image.NEAREST)\n",
        "        else:\n",
        "            img = img.resize(self.target_size, Image.BILINEAR)\n",
        "        \n",
        "        return np.array(img), img_array.shape, img_array.dtype\n",
        "    \n",
        "    def _load_and_resize_with_cv2(self, path, is_mask=False):\n",
        "        \"\"\"Load and resize using OpenCV with error handling\"\"\"\n",
        "        # Load image\n",
        "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Failed to load image: {path}\")\n",
        "        \n",
        "        original_shape = img.shape\n",
        "        original_dtype = img.dtype\n",
        "        \n",
        "        # Handle special cases before resize\n",
        "        if len(img.shape) > 3:\n",
        "            # Multi-page TIFF or 4D array - take first slice\n",
        "            img = img[0] if img.shape[0] > 1 else img.squeeze(0)\n",
        "        \n",
        "        # Ensure image is 2D or 3D\n",
        "        if len(img.shape) == 1:\n",
        "            raise ValueError(f\"1D image found at {path}\")\n",
        "        \n",
        "        # Convert to a dtype that OpenCV can handle for resize\n",
        "        resize_dtype_map = {\n",
        "            np.uint16: cv2.CV_16U,\n",
        "            np.int16: cv2.CV_16S,\n",
        "            np.uint8: cv2.CV_8U,\n",
        "            np.float32: cv2.CV_32F,\n",
        "            np.float64: cv2.CV_32F  # Convert float64 to float32\n",
        "        }\n",
        "        \n",
        "        # Convert problematic dtypes\n",
        "        if img.dtype == np.float64:\n",
        "            img = img.astype(np.float32)\n",
        "        elif img.dtype not in resize_dtype_map:\n",
        "            # Convert unknown types to float32\n",
        "            img = img.astype(np.float32)\n",
        "        \n",
        "        # Resize\n",
        "        try:\n",
        "            interpolation = cv2.INTER_NEAREST if is_mask else cv2.INTER_LINEAR\n",
        "            img_resized = cv2.resize(img, self.target_size, interpolation=interpolation)\n",
        "        except cv2.error as e:\n",
        "            print(f\"OpenCV resize failed for {path}\")\n",
        "            print(f\"Shape: {img.shape}, dtype: {img.dtype}\")\n",
        "            raise e\n",
        "        \n",
        "        return img_resized, original_shape, original_dtype\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_fname, mask_fname = self.image_mask_pairs[idx]\n",
        "        \n",
        "        img_path = os.path.join(self.image_dir, img_fname)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_fname)\n",
        "        \n",
        "        try:\n",
        "            # Load and resize images\n",
        "            if self.use_pil:\n",
        "                image, orig_img_shape, orig_img_dtype = self._load_and_resize_with_pil(img_path, is_mask=False)\n",
        "                mask, orig_mask_shape, orig_mask_dtype = self._load_and_resize_with_pil(mask_path, is_mask=True)\n",
        "            else:\n",
        "                image, orig_img_shape, orig_img_dtype = self._load_and_resize_with_cv2(img_path, is_mask=False)\n",
        "                mask, orig_mask_shape, orig_mask_dtype = self._load_and_resize_with_cv2(mask_path, is_mask=True)\n",
        "            \n",
        "            # Debug info for first image\n",
        "            if idx == 0:\n",
        "                print(f\"\\n[First TIFF Image Info]\")\n",
        "                print(f\"Image path: {img_path}\")\n",
        "                print(f\"Original shape: {orig_img_shape}, dtype: {orig_img_dtype}\")\n",
        "                print(f\"After resize: {image.shape}, dtype: {image.dtype}\")\n",
        "                print(f\"Value range: [{image.min()}, {image.max()}]\")\n",
        "                print(f\"Mask unique values: {np.unique(mask)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing image {idx}: {img_fname}\")\n",
        "            print(f\"Error details: {str(e)}\")\n",
        "            # Try with PIL if OpenCV failed\n",
        "            if not self.use_pil:\n",
        "                print(\"Retrying with PIL...\")\n",
        "                self.use_pil = True\n",
        "                return self.__getitem__(idx)\n",
        "            else:\n",
        "                raise\n",
        "        \n",
        "        # Ensure mask is 2D\n",
        "        if len(mask.shape) == 3:\n",
        "            mask = mask[:, :, 0]\n",
        "        elif len(mask.shape) != 2:\n",
        "            raise ValueError(f\"Unexpected mask shape: {mask.shape}\")\n",
        "        \n",
        "        # Normalize image based on data type\n",
        "        image = image.astype(np.float32)\n",
        "        if orig_img_dtype == np.uint16 or image.max() > 255:\n",
        "            # 16-bit image\n",
        "            image = image / 65535.0\n",
        "        elif orig_img_dtype == np.uint8 or (image.max() <= 255 and image.min() >= 0):\n",
        "            # 8-bit image\n",
        "            image = image / 255.0\n",
        "        else:\n",
        "            # Other types - normalize to [0, 1]\n",
        "            img_min, img_max = image.min(), image.max()\n",
        "            if img_max > img_min:\n",
        "                image = (image - img_min) / (img_max - img_min)\n",
        "        \n",
        "        # Ensure values are in [0, 1]\n",
        "        image = np.clip(image, 0, 1)\n",
        "        \n",
        "        # Convert mask to binary\n",
        "        mask = (mask > 0).astype(np.int64)\n",
        "        \n",
        "        # Handle channel dimension\n",
        "        if len(image.shape) == 2:\n",
        "            # Grayscale: (H, W) -> (1, H, W)\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "        else:\n",
        "            # Multi-channel: (H, W, C) -> (C, H, W)\n",
        "            image = image.transpose(2, 0, 1)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        image_tensor = torch.from_numpy(image.copy()).float()  # .copy() ensures contiguous memory\n",
        "        mask_tensor = torch.from_numpy(mask.copy()).long()\n",
        "        \n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "\n",
        "# Utility function to diagnose TIFF files\n",
        "def diagnose_tiff_files(image_dir, mask_dir, num_samples=5):\n",
        "    \"\"\"Diagnose TIFF files to understand their format\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"TIFF File Diagnosis\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    valid_exts = ('.tif', '.tiff')\n",
        "    files = [f for f in os.listdir(image_dir) if f.lower().endswith(valid_exts)]\n",
        "    \n",
        "    for i, fname in enumerate(files[:num_samples]):\n",
        "        print(f\"\\nFile {i+1}: {fname}\")\n",
        "        img_path = os.path.join(image_dir, fname)\n",
        "        \n",
        "        # Try OpenCV\n",
        "        try:\n",
        "            img_cv = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "            if img_cv is not None:\n",
        "                print(f\"  OpenCV: shape={img_cv.shape}, dtype={img_cv.dtype}, \"\n",
        "                      f\"range=[{img_cv.min()}, {img_cv.max()}]\")\n",
        "            else:\n",
        "                print(\"  OpenCV: Failed to load\")\n",
        "        except Exception as e:\n",
        "            print(f\"  OpenCV: Error - {str(e)}\")\n",
        "        \n",
        "        # Try PIL\n",
        "        try:\n",
        "            img_pil = Image.open(img_path)\n",
        "            img_array = np.array(img_pil)\n",
        "            print(f\"  PIL: mode={img_pil.mode}, size={img_pil.size}, \"\n",
        "                  f\"array_shape={img_array.shape}, dtype={img_array.dtype}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  PIL: Error - {str(e)}\")\n",
        "\n",
        "\n",
        "# Test function\n",
        "def test_dataset(image_dir, mask_dir, use_pil=False):\n",
        "    \"\"\"Test the dataset with both OpenCV and PIL\"\"\"\n",
        "    print(f\"\\nTesting dataset with {'PIL' if use_pil else 'OpenCV'}...\")\n",
        "    \n",
        "    dataset = CellSegmentationDatasetCV2(\n",
        "        image_dir, mask_dir, \n",
        "        target_size=(512, 512),\n",
        "        use_pil=use_pil\n",
        "    )\n",
        "    \n",
        "    # Test first few samples\n",
        "    for i in range(min(3, len(dataset))):\n",
        "        try:\n",
        "            img, mask = dataset[i]\n",
        "            print(f\"Sample {i}: Image {img.shape}, Mask {mask.shape} ✓\")\n",
        "        except Exception as e:\n",
        "            print(f\"Sample {i}: Failed - {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    image_dir = TRAIN_PATH\n",
        "    mask_dir = TRAIN_LABELS_PATH\n",
        "    \n",
        "    # First, diagnose the TIFF files\n",
        "    diagnose_tiff_files(image_dir, mask_dir)\n",
        "    \n",
        "    # Test with OpenCV\n",
        "    if not test_dataset(image_dir, mask_dir, use_pil=False):\n",
        "        print(\"\\nOpenCV failed, trying PIL...\")\n",
        "        test_dataset(image_dir, mask_dir, use_pil=True)\n",
        "\n",
        "# import cv2\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# import os\n",
        "\n",
        "# class CellSegmentationDatasetCV2(Dataset):\n",
        "#     def __init__(self, image_dir, mask_dir, target_size=(128, 128)):\n",
        "#         self.image_dir = image_dir\n",
        "#         self.mask_dir = mask_dir\n",
        "#         self.target_size = target_size\n",
        "\n",
        "#         # Uwzględnij tylko .tif i .tiff\n",
        "#         valid_exts = ('.tif', '.tiff')\n",
        "\n",
        "#         # Zbierz dostępne obrazy i sprawdź, czy odpowiadające maski istnieją\n",
        "#         self.image_mask_pairs = []\n",
        "#         for fname in os.listdir(image_dir):\n",
        "#             if fname.lower().endswith(valid_exts):\n",
        "#                 name, _ = os.path.splitext(fname)\n",
        "#                 # Zakładamy, że maska ma \"_label\" dodane\n",
        "#                 for ext in valid_exts:\n",
        "#                     mask_fname = f\"{name}_label{ext}\"\n",
        "#                     mask_path = os.path.join(mask_dir, mask_fname)\n",
        "#                     if os.path.isfile(mask_path):\n",
        "#                         self.image_mask_pairs.append((fname, mask_fname))\n",
        "#                         break  # znajdziono odpowiednią maskę, więc nie trzeba sprawdzać dalej\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_mask_pairs)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path, mask_path = self.image_mask_pairs[idx]\n",
        "\n",
        "#         img_path = os.path.join(self.image_dir, img_path)\n",
        "#         mask_path = os.path.join(self.mask_dir, mask_path)\n",
        "\n",
        "#         image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "#         mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "#         if image is None:\n",
        "#             raise ValueError(f\"Nie udało się wczytać obrazu: {img_path}\")\n",
        "#         if mask is None:\n",
        "#             raise ValueError(f\"Nie udało się wczytać maski: {mask_path}\")\n",
        "\n",
        "#         print(f\"Image shape before resize: {image.shape}, dtype: {image.dtype}\")\n",
        "#         print(f\"Mask shape before resize: {mask.shape}, dtype: {mask.dtype}\")\n",
        "\n",
        "#         return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1SqA_J56Ujk",
        "outputId": "61f4755e-6f2b-46b6-a2fc-a4e393b9b833"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ImageTiffDataset.__init__() missing 2 required positional arguments: 'cache_dir' and 'filenames'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[63], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Image shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Mask shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Załaduj dane\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageTiffDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_LABELS_PATH\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO: more args needed\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(len(train_dataset))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ImageTiffDataset(VAL_PATH, VAL_LABELS_PATH)\n",
            "\u001b[0;31mTypeError\u001b[0m: ImageTiffDataset.__init__() missing 2 required positional arguments: 'cache_dir' and 'filenames'"
          ]
        }
      ],
      "source": [
        "def print_all_shapes(name, dataset):\n",
        "    print(f\"\\n{name} — liczba par: {len(dataset)}\")\n",
        "    for idx in range(len(dataset)):\n",
        "        img, mask = dataset[idx]\n",
        "        print(f\"[{idx}] Image shape: {img.shape}, Mask shape: {mask.shape}\")\n",
        "\n",
        "# Załaduj dane\n",
        "train_dataset = ImageTiffDataset(TRAIN_PATH, TRAIN_LABELS_PATH) # TODO: more args needed or change init\n",
        "#print(len(train_dataset))\n",
        "val_dataset = ImageTiffDataset(VAL_PATH, VAL_LABELS_PATH)\n",
        "test_dataset = ImageTiffDataset(TEST_PATH, TEST_LABELS_PATH)\n",
        "\n",
        "# Wypisz wszystkie wymiary\n",
        "print_all_shapes(\"Train\", train_dataset)\n",
        "print_all_shapes(\"Validation\", val_dataset)\n",
        "print_all_shapes(\"Test\", test_dataset)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def analyze_mask_values(dataset, name):\n",
        "    print(f\"\\nAnaliza masek w zbiorze: {name}\")\n",
        "    unique_values = set()\n",
        "    for idx in range(len(dataset)):\n",
        "        _, mask = dataset[idx]\n",
        "        vals = np.unique(mask)\n",
        "        unique_values.update(vals.tolist())\n",
        "        if idx < 5:  # pokaż kilka przykładów na start\n",
        "            print(f\"[{idx}] Unikalne wartości: {vals}\")\n",
        "    print(f\"Zbiór wszystkich unikalnych wartości w maskach ({name}): {sorted(unique_values)}\")\n",
        "\n",
        "analyze_mask_values(train_dataset, \"Train\")\n",
        "analyze_mask_values(val_dataset, \"Validation\")\n",
        "analyze_mask_values(test_dataset, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJZYTlqg9orB",
        "outputId": "8c0f710c-7f9c-445b-9578-9e5e50bc393e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[43. 42. 41. ... 44. 42. 43.]\n",
            " [43. 40. 40. ... 42. 43. 42.]\n",
            " [43. 41. 40. ... 42. 43. 41.]\n",
            " ...\n",
            " [51. 48. 46. ... 42. 43. 42.]\n",
            " [50. 48. 47. ... 43. 44. 42.]\n",
            " [49. 49. 50. ... 45. 46. 46.]]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "img_path = \"data_train/Training-labeled/images/cell_00302.tiff\"\n",
        "image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "print(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRyc9tw59tLD"
      },
      "source": [
        "## Basic UNet (UNet_1) from https://rpubs.com/eR_ic/unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp-DsAYo-L4d",
        "outputId": "6e7822a5-3cd9-4206-b6de-8fa270dd4047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "              ReLU-2           [-1, 64, 32, 32]               0\n",
            "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
            "              ReLU-4           [-1, 64, 32, 32]               0\n",
            "double_convolution-5           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-6           [-1, 64, 16, 16]               0\n",
            "       down_sample-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "              ReLU-9          [-1, 128, 16, 16]               0\n",
            "           Conv2d-10          [-1, 128, 16, 16]         147,584\n",
            "             ReLU-11          [-1, 128, 16, 16]               0\n",
            "double_convolution-12          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-13            [-1, 128, 8, 8]               0\n",
            "      down_sample-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "             ReLU-16            [-1, 256, 8, 8]               0\n",
            "           Conv2d-17            [-1, 256, 8, 8]         590,080\n",
            "             ReLU-18            [-1, 256, 8, 8]               0\n",
            "double_convolution-19            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-20            [-1, 256, 4, 4]               0\n",
            "      down_sample-21            [-1, 256, 4, 4]               0\n",
            "           Conv2d-22            [-1, 512, 4, 4]       1,180,160\n",
            "             ReLU-23            [-1, 512, 4, 4]               0\n",
            "           Conv2d-24            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-25            [-1, 512, 4, 4]               0\n",
            "double_convolution-26            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-27            [-1, 512, 2, 2]               0\n",
            "      down_sample-28            [-1, 512, 2, 2]               0\n",
            "           Conv2d-29           [-1, 1024, 2, 2]       4,719,616\n",
            "             ReLU-30           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-31           [-1, 1024, 2, 2]       9,438,208\n",
            "             ReLU-32           [-1, 1024, 2, 2]               0\n",
            "double_convolution-33           [-1, 1024, 2, 2]               0\n",
            "  ConvTranspose2d-34            [-1, 512, 4, 4]       2,097,664\n",
            "        up_sample-35            [-1, 512, 4, 4]               0\n",
            "crop_and_concatenate-36           [-1, 1024, 4, 4]               0\n",
            "           Conv2d-37            [-1, 512, 4, 4]       4,719,104\n",
            "             ReLU-38            [-1, 512, 4, 4]               0\n",
            "           Conv2d-39            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-40            [-1, 512, 4, 4]               0\n",
            "double_convolution-41            [-1, 512, 4, 4]               0\n",
            "  ConvTranspose2d-42            [-1, 256, 8, 8]         524,544\n",
            "        up_sample-43            [-1, 256, 8, 8]               0\n",
            "crop_and_concatenate-44            [-1, 512, 8, 8]               0\n",
            "           Conv2d-45            [-1, 256, 8, 8]       1,179,904\n",
            "             ReLU-46            [-1, 256, 8, 8]               0\n",
            "           Conv2d-47            [-1, 256, 8, 8]         590,080\n",
            "             ReLU-48            [-1, 256, 8, 8]               0\n",
            "double_convolution-49            [-1, 256, 8, 8]               0\n",
            "  ConvTranspose2d-50          [-1, 128, 16, 16]         131,200\n",
            "        up_sample-51          [-1, 128, 16, 16]               0\n",
            "crop_and_concatenate-52          [-1, 256, 16, 16]               0\n",
            "           Conv2d-53          [-1, 128, 16, 16]         295,040\n",
            "             ReLU-54          [-1, 128, 16, 16]               0\n",
            "           Conv2d-55          [-1, 128, 16, 16]         147,584\n",
            "             ReLU-56          [-1, 128, 16, 16]               0\n",
            "double_convolution-57          [-1, 128, 16, 16]               0\n",
            "  ConvTranspose2d-58           [-1, 64, 32, 32]          32,832\n",
            "        up_sample-59           [-1, 64, 32, 32]               0\n",
            "crop_and_concatenate-60          [-1, 128, 32, 32]               0\n",
            "           Conv2d-61           [-1, 64, 32, 32]          73,792\n",
            "             ReLU-62           [-1, 64, 32, 32]               0\n",
            "           Conv2d-63           [-1, 64, 32, 32]          36,928\n",
            "             ReLU-64           [-1, 64, 32, 32]               0\n",
            "double_convolution-65           [-1, 64, 32, 32]               0\n",
            "           Conv2d-66            [-1, 1, 32, 32]              65\n",
            "================================================================\n",
            "Total params: 31,031,745\n",
            "Trainable params: 31,031,745\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 13.76\n",
            "Params size (MB): 118.38\n",
            "Estimated Total Size (MB): 132.15\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# This code implements a U-Net model for semantic segmentation from the paper U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional\n",
        "\n",
        "# Implement the double 3X3 convolution blocks\n",
        "# The original paper did not use padding, but we will use padding to keep the image size the same\n",
        "\n",
        "class double_convolution(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the double convolution block which consists of two 3X3 convolution layers,\n",
        "    each followed by a ReLU activation function.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels): # Initialize the class\n",
        "        super().__init__() # Initialize the parent class\n",
        "\n",
        "        # First 3X3 convolution layer\n",
        "        self.first_cnn = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        # Second 3X3 convolution layer\n",
        "        self.second_cnn = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "    # Pass the input through the double convolution block\n",
        "    def forward(self, x):\n",
        "        x = self.first_cnn(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.act2(self.second_cnn(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Implement the Downsample block that occurs after each double convolution block\n",
        "class down_sample(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the downsample block which consists of a Max Pooling layer with a kernel size of 2.\n",
        "    The Max Pooling layer halves the image size reducing the spatial resolution of the feature maps\n",
        "    while retaining the most important features.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the downsample block\n",
        "    def forward(self, x):\n",
        "        x = self.max_pool(x)\n",
        "        return x\n",
        "\n",
        "# Implement the UpSample block that occurs in the decoder part of the network\n",
        "class up_sample(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the upsample block which consists of a convolution transpose layer with a kernel size of 2.\n",
        "    The convolution transpose layer doubles the image size increasing the spatial resolution of the feature maps\n",
        "    while retaining the learned features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution transpose layer\n",
        "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the upsample block\n",
        "    def forward(self, x):\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "# Implement the crop and concatenate block that occurs in the decoder part of the network\n",
        "# This block concatenates the output of the upsample block with the output of the corresponding downsample block\n",
        "# The output of the crop and concatenate block is then passed through a double convolution block\n",
        "class crop_and_concatenate(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the crop and concatenate block which combines the output of the upsample block\n",
        "    with the corresponding features from the contracting path through skip connections,\n",
        "    allowing the network to recover the fine-grained details lost during downsampling\n",
        "    and produce a high-resolution output segmentation map.\n",
        "    \"\"\"\n",
        "    # def forward(self, upsampled, bypass):\n",
        "    #     # Crop the feature map from the contacting path to match the size of the upsampled feature map\n",
        "    #     bypass = torchvision.transforms.functional.center_crop(img = bypass, output_size = [upsampled.shape[2], upsampled.shape[3]])\n",
        "    #     # Concatenate the upsampled feature map with the cropped feature map from the contracting path\n",
        "    #     x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
        "    #     return x\n",
        "    # Alternatively crop the upsampled feature map to match the size of the feature map from the contracting path\n",
        "    def forward(self, upsampled, bypass):\n",
        "        upsampled = torchvision.transforms.functional.resize(img = upsampled, size = bypass.shape[2:], antialias=True)\n",
        "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
        "        return x\n",
        "\n",
        "# m = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "# input = torch.randn(1, 1024, 28, 28)\n",
        "# m(input).shape\n",
        "\n",
        "# m = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "# xx = torch.randn(1, 1, 143, 143)\n",
        "# m(xx).shape\n",
        "\n",
        "## Implement the UNet architecture\n",
        "class UNet(nn.Module):\n",
        "    # in_channels: number of channels in the input image\n",
        "    # out_channels: number of channels in the output image\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the contracting path: convolution blocks followed by downsample blocks\n",
        "        self.down_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(in_channels, 64), (64, 128), (128, 256), (256, 512)]) # List of downsample blocks\n",
        "\n",
        "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
        "\n",
        "        # Define the bottleneck layer\n",
        "        self.bottleneck = double_convolution(in_channels = 512, out_channels = 1024)\n",
        "\n",
        "        # Define the expanding path: upsample blocks followed by convolution blocks\n",
        "        self.up_samples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of upsample blocks\n",
        "\n",
        "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
        "\n",
        "        self.up_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of convolution blocks\n",
        "\n",
        "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
        "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
        "        # while leaving the spatial dimensions unchanged.\n",
        "        self.final_conv = nn.Conv2d(in_channels = 64, out_channels = out_channels, kernel_size = 1)\n",
        "\n",
        "    # Pass the input through the UNet architecture\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the contacting path\n",
        "        skip_connections = [] # List to store the outputs of the downsample blocks\n",
        "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
        "            x = down_conv(x)\n",
        "            skip_connections.append(x)\n",
        "            x = down_sample(x)\n",
        "\n",
        "        # Pass the output of the contacting path through the bottleneck layer\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Pass the output of the bottleneck layer through the expanding path\n",
        "        skip_connections = skip_connections[::-1] # Reverse the list of skip connections\n",
        "        for up_sample, concat, up_conv in zip(self.up_samples, self.concat, self.up_conv):\n",
        "            x = up_sample(x)\n",
        "            x = concat(x, skip_connections.pop(0)) # Remove the first element from the list of skip connections\n",
        "            x = up_conv(x)\n",
        "\n",
        "        # Pass the output of the expanding path through the final convolution layer\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "# Sanity check for the model\n",
        "import torchsummary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_channels = 3, out_channels = 1).to(device)\n",
        "dummy_input = torch.randn((1, 3, 32, 32)).to(device)\n",
        "mask = model(dummy_input)\n",
        "mask.shape\n",
        "\n",
        "# See how data flows through the network\n",
        "torchsummary.summary(model, input_size = (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWPypoOLK6jd"
      },
      "source": [
        "## Training UNet_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WjDQFqN_LHWz"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    loss_fn: nn.Module,\n",
        "    epoch: int = 0,\n",
        "    log_interval: int = 10\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        model: the neural network (e.g., UNet)\n",
        "        dataloader: DataLoader providing training batches\n",
        "        optimizer: optimizer used for weight updates (e.g., Adam)\n",
        "        device: device to run computations on (CPU or GPU)\n",
        "        loss_fn: loss function (e.g., CrossEntropyLoss)\n",
        "        epoch: current epoch number (used for logging)\n",
        "        log_interval: print status every N batches\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: average training loss over the epoch\n",
        "        accuracy: pixel-level classification accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs = inputs.to(device)           # Input shape: (B, C, H, W)\n",
        "        targets = targets.to(device)         # Target shape: (B, H, W), where each pixel has a class label\n",
        "\n",
        "        optimizer.zero_grad()                # Clear gradients from previous step\n",
        "        outputs = model(inputs)              # Output shape: (B, num_classes, H, W)\n",
        "\n",
        "        loss = loss_fn(outputs, targets)     # Calculate loss\n",
        "        loss.backward()                      # Backpropagation\n",
        "        optimizer.step()                     # Update model weights\n",
        "\n",
        "        running_loss += loss.item()          # Accumulate batch loss\n",
        "\n",
        "        # Compute pixel-wise accuracy\n",
        "        preds = outputs.argmax(dim=1)        # Get predicted class per pixel, shape: (B, H, W)\n",
        "        correct_pixels += (preds == targets).sum().item()  # Count correct predictions\n",
        "        total_pixels += targets.numel()      # Total number of pixels\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                f\"[Epoch {epoch}] Batch {batch_idx}/{len(dataloader)} - \"\n",
        "                f\"Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    # Average loss and overall accuracy for the epoch\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = 100.0 * correct_pixels / total_pixels\n",
        "\n",
        "    print(f\"Epoch {epoch} finished. Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    device: torch.device,\n",
        "    loss_fn: nn.Module\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the validation set.\n",
        "\n",
        "    Args:\n",
        "        model: the neural network\n",
        "        dataloader: DataLoader for validation data\n",
        "        device: device to run computations on\n",
        "        loss_fn: same loss function as used in training\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: average validation loss\n",
        "        accuracy: pixel-wise classification accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval() # Set model to evaluation mode (disables dropout, etc.)\n",
        "    running_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)              # Forward pass\n",
        "        loss = loss_fn(outputs, targets)     # Compute loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)        # Get predicted class per pixel\n",
        "        correct_pixels += (preds == targets).sum().item()\n",
        "        total_pixels += targets.numel()\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = 100.0 * correct_pixels / total_pixels\n",
        "    print(f\"Validation - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "kFwbsldhL41-",
        "outputId": "7968c21a-7a50-4d1b-8df3-e812ee840a60"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [1608] and output size of [512, 512]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[51], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Training loop over all epochs\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Train the model for one epoch on the training dataset\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Evaluate the model on the validation dataset after each epoch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     validate_one_epoch(model, dataloader_val_cv, device, loss_fn)\n",
            "Cell \u001b[0;32mIn[50], line 34\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device, loss_fn, epoch, log_interval)\u001b[0m\n\u001b[1;32m     31\u001b[0m correct_pixels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m total_pixels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Input shape: (B, C, H, W)\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Target shape: (B, H, W), where each pixel has a class label\u001b[39;49;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[28], line 114\u001b[0m, in \u001b[0;36mImageTiffDatasetCV.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Apply optional transform to the target mask\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m--> 114\u001b[0m     target_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Save the processed mask tensor to cache\u001b[39;00m\n\u001b[1;32m    116\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(target_tensor, target_cache)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torchvision/transforms/functional.py:479\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:467\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/functional.py:4566\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   4565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m-> 4566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4567\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4568\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4571\u001b[0m         )\n\u001b[1;32m   4572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m   4573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
            "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [1608] and output size of [512, 512]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# in_channels=3 for RGB images, out_channels=2 for binary segmentation with 2 classes (background vs object)\n",
        "model = UNet(in_channels=3, out_channels=2).to(device)\n",
        "\n",
        "# Use the Adam optimizer with a small learning rate (good starting point for U-Net training)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Use CrossEntropyLoss, which is suitable for multi-class (including binary) classification problems\n",
        "# It expects raw logits from the model and class indices (not one-hot) as targets\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 30\n",
        "\n",
        "# Training loop over all epochs\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # Train the model for one epoch on the training dataset\n",
        "    train_one_epoch(model, dataloader_train_cv, optimizer, device, loss_fn, epoch)\n",
        "\n",
        "    # Evaluate the model on the validation dataset after each epoch\n",
        "    validate_one_epoch(model, dataloader_val_cv, device, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AWEoikrlhH8",
        "outputId": "7b9155bf-0d23-4860-bc81-8a0fe4f39fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File exists: True\n",
            "File size: 8000272 bytes\n",
            "<class 'numpy.ndarray'> (1000, 1000)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "path = \"data_train/Training-labeled/images/cell_00311.tiff\"\n",
        "print(f\"File exists: {os.path.exists(path)}\")\n",
        "print(f\"File size: {os.path.getsize(path)} bytes\")\n",
        "\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "print(type(img), img.shape if img is not None else \"Can't load image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmTklIAGl_yI",
        "outputId": "84c7eaf1-5fd6-46a3-d095-ac7ebd929139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sprawdzam 1000 obrazów i 1000 masek...\n",
            "[BŁĄD OBRAZU] cell_00301.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00301.tiff'\n",
            "[BŁĄD OBRAZU] cell_00302.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00302.tiff'\n",
            "[BŁĄD OBRAZU] cell_00303.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00303.tiff'\n",
            "[BŁĄD OBRAZU] cell_00304.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00304.tiff'\n",
            "[BŁĄD OBRAZU] cell_00305.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00305.tiff'\n",
            "[BŁĄD OBRAZU] cell_00306.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00306.tiff'\n",
            "[BŁĄD OBRAZU] cell_00307.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00307.tiff'\n",
            "[BŁĄD OBRAZU] cell_00308.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00308.tiff'\n",
            "[BŁĄD OBRAZU] cell_00309.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00309.tiff'\n",
            "[BŁĄD OBRAZU] cell_00310.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00310.tiff'\n",
            "[BŁĄD OBRAZU] cell_00311.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00311.tiff'\n",
            "[BŁĄD OBRAZU] cell_00312.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00312.tiff'\n",
            "[BŁĄD OBRAZU] cell_00313.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00313.tiff'\n",
            "[BŁĄD OBRAZU] cell_00314.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00314.tiff'\n",
            "[BŁĄD OBRAZU] cell_00315.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00315.tiff'\n",
            "[BŁĄD OBRAZU] cell_00316.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00316.tiff'\n",
            "[BŁĄD OBRAZU] cell_00501.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00501.tif'\n",
            "[BŁĄD OBRAZU] cell_00502.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00502.tif'\n",
            "[BŁĄD OBRAZU] cell_00503.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00503.tif'\n",
            "[BŁĄD OBRAZU] cell_00504.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00504.tif'\n",
            "[BŁĄD OBRAZU] cell_00505.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00505.tif'\n",
            "[BŁĄD OBRAZU] cell_00506.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00506.tif'\n",
            "[BŁĄD OBRAZU] cell_00507.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00507.tif'\n",
            "\n",
            "Podsumowanie:\n",
            "- Uszkodzone obrazy: 23\n",
            "- Uszkodzone maski: 0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "def validate_tiff_dataset(image_dir, label_dir):\n",
        "    broken_images = []\n",
        "    broken_labels = []\n",
        "\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    label_files = sorted(os.listdir(label_dir))\n",
        "\n",
        "    print(f\"Sprawdzam {len(image_files)} obrazów i {len(label_files)} masek...\")\n",
        "\n",
        "    for filename in image_files:\n",
        "        img_path = os.path.join(image_dir, filename)\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                img.verify()  # tylko weryfikuje (nie ładuje całkowicie)\n",
        "        except (UnidentifiedImageError, OSError, FileNotFoundError) as e:\n",
        "            print(f\"[BŁĄD OBRAZU] {filename}: {e}\")\n",
        "            broken_images.append(filename)\n",
        "\n",
        "    for filename in label_files:\n",
        "        label_path = os.path.join(label_dir, filename)\n",
        "        try:\n",
        "            with Image.open(label_path) as img:\n",
        "                img.verify()\n",
        "        except (UnidentifiedImageError, OSError, FileNotFoundError) as e:\n",
        "            print(f\"[BŁĄD MASKI] {filename}: {e}\")\n",
        "            broken_labels.append(filename)\n",
        "\n",
        "    print(\"\\nPodsumowanie:\")\n",
        "    print(f\"- Uszkodzone obrazy: {len(broken_images)}\")\n",
        "    print(f\"- Uszkodzone maski: {len(broken_labels)}\")\n",
        "\n",
        "    return broken_images, broken_labels\n",
        "\n",
        "bad_imgs, bad_labels = validate_tiff_dataset(TRAIN_PATH, TRAIN_LABELS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "348D8uo9raMa",
        "outputId": "e473366b-1eb4-4dd1-ab1f-f098e449377d"
      },
      "outputs": [
        {
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file 'data_train/Training-labeled/images/cell_00501.tif'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fd911247a252>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_train/Training-labeled/images/cell_00501.tif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3570\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3572\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file 'data_train/Training-labeled/images/cell_00501.tif'"
          ]
        }
      ],
      "source": [
        "Image.open('data_train/Training-labeled/images/cell_00501.tif').verify()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD2B-B0NwdlQ",
        "outputId": "5e93b7b5-52d3-477a-9348-d8ab1448d031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "target_np = cv2.imread('data_train/Training-labeled/labels/cell_00490_label.tiff', cv2.IMREAD_UNCHANGED)\n",
        "print(target_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwUzrjwtrMn2",
        "outputId": "fec20442-2524-4aa9-835a-c7b57ef0c158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cell_00301.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00302.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00303.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00304.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00305.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00306.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00307.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00308.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00309.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00310.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00311.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00312.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00313.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00314.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00315.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00316.tiff OK — można użyć OpenCV zamiast PIL\n",
            "cell_00501.tif OK — można użyć OpenCV zamiast PIL\n",
            "cell_00502.tif OK — można użyć OpenCV zamiast PIL\n",
            "cell_00503.tif OK — można użyć OpenCV zamiast PIL\n",
            "cell_00504.tif OK — można użyć OpenCV zamiast PIL\n",
            "cell_00505.tif OK — można użyć OpenCV zamiast PIL\n",
            "cell_00506.tif OK — można użyć OpenCV zamiast PIL\n",
            "cell_00507.tif OK — można użyć OpenCV zamiast PIL\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "for img in bad_imgs:\n",
        "    path = os.path.join(TRAIN_PATH, img)\n",
        "    image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "    if image is None:\n",
        "        print(f\"{img} nie da się otworzyć przez OpenCV\")\n",
        "    else:\n",
        "        print(f\"{img} OK — można użyć OpenCV zamiast PIL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNFh-xjVPuVY"
      },
      "source": [
        "## Testing UNet_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANmKIkz-QEjM"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_unet(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)             # (B, C, H, W)\n",
        "        masks = masks.to(device)               # (B, H, W) — ground truth z etykietami klas\n",
        "\n",
        "        outputs = model(images)                # (B, num_classes, H, W)\n",
        "        preds = outputs.argmax(dim=1)          # (B, H, W)\n",
        "\n",
        "        total_correct += (preds == masks).sum().item()\n",
        "        total_pixels += masks.numel()\n",
        "\n",
        "    accuracy = 100.0 * total_correct / total_pixels\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_predictions(model, dataloader, device, num_examples=3):\n",
        "    model.eval()\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        for i in range(min(num_examples, images.size(0))):\n",
        "            img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "            mask = masks[i].cpu().numpy()\n",
        "            pred = preds[i].cpu().numpy()\n",
        "\n",
        "            fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "            axs[0].imshow(img)\n",
        "            axs[0].set_title(\"Input Image\")\n",
        "            axs[1].imshow(mask)\n",
        "            axs[1].set_title(\"Ground Truth\")\n",
        "            axs[2].imshow(pred)\n",
        "            axs[2].set_title(\"Prediction\")\n",
        "            plt.show()\n",
        "        break  # tylko jedna batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "bEge6jW3QLLR",
        "outputId": "2c90ed9b-b3bd-43b3-821b-5e9da8518600"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [1, 566, 630] at entry 0 and [3, 944, 1266] at entry 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-31904e00ea73>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvisualize_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-3128a1670e3f>\u001b[0m in \u001b[0;36mtest_unet\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# (B, C, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (B, H, W) — ground truth z etykietami klas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 566, 630] at entry 0 and [3, 944, 1266] at entry 1"
          ]
        }
      ],
      "source": [
        "test_unet(model, dataloader_test, device)\n",
        "visualize_predictions(model, dataloader_test, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkk-5CVkTSVD"
      },
      "source": [
        "## ResidualAttentionUnet (UNet_2) from https://rpubs.com/eR_ic/unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il9-awnFTyMW",
        "outputId": "448a3ffb-af65-4796-9297-18fc3a7b9cfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "         GroupNorm-2           [-1, 64, 32, 32]             128\n",
            "              SiLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "         GroupNorm-5           [-1, 64, 32, 32]             128\n",
            "              SiLU-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7           [-1, 64, 32, 32]             256\n",
            "    residual_block-8           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-9           [-1, 64, 16, 16]               0\n",
            "      down_sample-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]          73,856\n",
            "        GroupNorm-12          [-1, 128, 16, 16]             256\n",
            "             SiLU-13          [-1, 128, 16, 16]               0\n",
            "           Conv2d-14          [-1, 128, 16, 16]         147,584\n",
            "        GroupNorm-15          [-1, 128, 16, 16]             256\n",
            "             SiLU-16          [-1, 128, 16, 16]               0\n",
            "           Conv2d-17          [-1, 128, 16, 16]           8,320\n",
            "   residual_block-18          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-19            [-1, 128, 8, 8]               0\n",
            "      down_sample-20            [-1, 128, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         295,168\n",
            "        GroupNorm-22            [-1, 256, 8, 8]             512\n",
            "             SiLU-23            [-1, 256, 8, 8]               0\n",
            "           Conv2d-24            [-1, 256, 8, 8]         590,080\n",
            "        GroupNorm-25            [-1, 256, 8, 8]             512\n",
            "             SiLU-26            [-1, 256, 8, 8]               0\n",
            "           Conv2d-27            [-1, 256, 8, 8]          33,024\n",
            "   residual_block-28            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-29            [-1, 256, 4, 4]               0\n",
            "      down_sample-30            [-1, 256, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       1,180,160\n",
            "        GroupNorm-32            [-1, 512, 4, 4]           1,024\n",
            "             SiLU-33            [-1, 512, 4, 4]               0\n",
            "           Conv2d-34            [-1, 512, 4, 4]       2,359,808\n",
            "        GroupNorm-35            [-1, 512, 4, 4]           1,024\n",
            "             SiLU-36            [-1, 512, 4, 4]               0\n",
            "           Conv2d-37            [-1, 512, 4, 4]         131,584\n",
            "   residual_block-38            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-39            [-1, 512, 2, 2]               0\n",
            "      down_sample-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41           [-1, 1024, 2, 2]       4,719,616\n",
            "        GroupNorm-42           [-1, 1024, 2, 2]           2,048\n",
            "             SiLU-43           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-44           [-1, 1024, 2, 2]       9,438,208\n",
            "        GroupNorm-45           [-1, 1024, 2, 2]           2,048\n",
            "             SiLU-46           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-47           [-1, 1024, 2, 2]         525,312\n",
            "   residual_block-48           [-1, 1024, 2, 2]               0\n",
            "  ConvTranspose2d-49            [-1, 512, 4, 4]       2,097,664\n",
            "        up_sample-50            [-1, 512, 4, 4]               0\n",
            "           Conv2d-51            [-1, 256, 4, 4]         131,328\n",
            "           Conv2d-52            [-1, 256, 4, 4]         131,328\n",
            "             SiLU-53            [-1, 256, 4, 4]               0\n",
            "           Conv2d-54              [-1, 1, 4, 4]             257\n",
            "          Sigmoid-55              [-1, 1, 4, 4]               0\n",
            "        GroupNorm-56            [-1, 512, 4, 4]           1,024\n",
            "             SiLU-57            [-1, 512, 4, 4]               0\n",
            "  attention_block-58            [-1, 512, 4, 4]               0\n",
            "  ConvTranspose2d-59            [-1, 256, 8, 8]         524,544\n",
            "        up_sample-60            [-1, 256, 8, 8]               0\n",
            "           Conv2d-61            [-1, 128, 8, 8]          32,896\n",
            "           Conv2d-62            [-1, 128, 8, 8]          32,896\n",
            "             SiLU-63            [-1, 128, 8, 8]               0\n",
            "           Conv2d-64              [-1, 1, 8, 8]             129\n",
            "          Sigmoid-65              [-1, 1, 8, 8]               0\n",
            "        GroupNorm-66            [-1, 256, 8, 8]             512\n",
            "             SiLU-67            [-1, 256, 8, 8]               0\n",
            "  attention_block-68            [-1, 256, 8, 8]               0\n",
            "  ConvTranspose2d-69          [-1, 128, 16, 16]         131,200\n",
            "        up_sample-70          [-1, 128, 16, 16]               0\n",
            "           Conv2d-71           [-1, 64, 16, 16]           8,256\n",
            "           Conv2d-72           [-1, 64, 16, 16]           8,256\n",
            "             SiLU-73           [-1, 64, 16, 16]               0\n",
            "           Conv2d-74            [-1, 1, 16, 16]              65\n",
            "          Sigmoid-75            [-1, 1, 16, 16]               0\n",
            "        GroupNorm-76          [-1, 128, 16, 16]             256\n",
            "             SiLU-77          [-1, 128, 16, 16]               0\n",
            "  attention_block-78          [-1, 128, 16, 16]               0\n",
            "  ConvTranspose2d-79           [-1, 64, 32, 32]          32,832\n",
            "        up_sample-80           [-1, 64, 32, 32]               0\n",
            "           Conv2d-81           [-1, 32, 32, 32]           2,080\n",
            "           Conv2d-82           [-1, 32, 32, 32]           2,080\n",
            "             SiLU-83           [-1, 32, 32, 32]               0\n",
            "           Conv2d-84            [-1, 1, 32, 32]              33\n",
            "          Sigmoid-85            [-1, 1, 32, 32]               0\n",
            "        GroupNorm-86           [-1, 64, 32, 32]             128\n",
            "             SiLU-87           [-1, 64, 32, 32]               0\n",
            "  attention_block-88           [-1, 64, 32, 32]               0\n",
            "  ConvTranspose2d-89            [-1, 512, 4, 4]       2,097,664\n",
            "        up_sample-90            [-1, 512, 4, 4]               0\n",
            "crop_and_concatenate-91           [-1, 1024, 4, 4]               0\n",
            "           Conv2d-92            [-1, 512, 4, 4]       4,719,104\n",
            "        GroupNorm-93            [-1, 512, 4, 4]           1,024\n",
            "             SiLU-94            [-1, 512, 4, 4]               0\n",
            "           Conv2d-95            [-1, 512, 4, 4]       2,359,808\n",
            "        GroupNorm-96            [-1, 512, 4, 4]           1,024\n",
            "             SiLU-97            [-1, 512, 4, 4]               0\n",
            "           Conv2d-98            [-1, 512, 4, 4]         524,800\n",
            "   residual_block-99            [-1, 512, 4, 4]               0\n",
            " ConvTranspose2d-100            [-1, 256, 8, 8]         524,544\n",
            "       up_sample-101            [-1, 256, 8, 8]               0\n",
            "crop_and_concatenate-102            [-1, 512, 8, 8]               0\n",
            "          Conv2d-103            [-1, 256, 8, 8]       1,179,904\n",
            "       GroupNorm-104            [-1, 256, 8, 8]             512\n",
            "            SiLU-105            [-1, 256, 8, 8]               0\n",
            "          Conv2d-106            [-1, 256, 8, 8]         590,080\n",
            "       GroupNorm-107            [-1, 256, 8, 8]             512\n",
            "            SiLU-108            [-1, 256, 8, 8]               0\n",
            "          Conv2d-109            [-1, 256, 8, 8]         131,328\n",
            "  residual_block-110            [-1, 256, 8, 8]               0\n",
            " ConvTranspose2d-111          [-1, 128, 16, 16]         131,200\n",
            "       up_sample-112          [-1, 128, 16, 16]               0\n",
            "crop_and_concatenate-113          [-1, 256, 16, 16]               0\n",
            "          Conv2d-114          [-1, 128, 16, 16]         295,040\n",
            "       GroupNorm-115          [-1, 128, 16, 16]             256\n",
            "            SiLU-116          [-1, 128, 16, 16]               0\n",
            "          Conv2d-117          [-1, 128, 16, 16]         147,584\n",
            "       GroupNorm-118          [-1, 128, 16, 16]             256\n",
            "            SiLU-119          [-1, 128, 16, 16]               0\n",
            "          Conv2d-120          [-1, 128, 16, 16]          32,896\n",
            "  residual_block-121          [-1, 128, 16, 16]               0\n",
            " ConvTranspose2d-122           [-1, 64, 32, 32]          32,832\n",
            "       up_sample-123           [-1, 64, 32, 32]               0\n",
            "crop_and_concatenate-124          [-1, 128, 32, 32]               0\n",
            "          Conv2d-125           [-1, 64, 32, 32]          73,792\n",
            "       GroupNorm-126           [-1, 64, 32, 32]             128\n",
            "            SiLU-127           [-1, 64, 32, 32]               0\n",
            "          Conv2d-128           [-1, 64, 32, 32]          36,928\n",
            "       GroupNorm-129           [-1, 64, 32, 32]             128\n",
            "            SiLU-130           [-1, 64, 32, 32]               0\n",
            "          Conv2d-131           [-1, 64, 32, 32]           8,256\n",
            "  residual_block-132           [-1, 64, 32, 32]               0\n",
            "          Conv2d-133            [-1, 1, 32, 32]              65\n",
            "================================================================\n",
            "Total params: 35,577,061\n",
            "Trainable params: 35,577,061\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 25.59\n",
            "Params size (MB): 135.72\n",
            "Estimated Total Size (MB): 161.32\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "# Define a Residual block\n",
        "class residual_block(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a residual block which consists of two convolution layers with group normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, n_groups = 8):\n",
        "        super().__init__()\n",
        "        # First convolution layer\n",
        "        self.first_conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.first_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
        "        self.act1 = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.second_conv = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.second_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
        "        self.act2 = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # If the number of input channels is not equal to the number of output channels,\n",
        "        # then use a 1X1 convolution layer to compensate for the difference in dimensions\n",
        "        # This allows the input to have the same dimensions as the output of the residual block\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1)\n",
        "        else:\n",
        "            # Pass the input as is\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    # Pass the input through the residual block\n",
        "    def forward(self, x):\n",
        "        # Store the input\n",
        "        input = x\n",
        "\n",
        "        # Pass input through the first convolution layer\n",
        "        x = self.act1(self.second_norm(self.first_conv(x)))\n",
        "\n",
        "        # Pass the output of the first convolution layer through the second convolution layer\n",
        "        x = self.act2(self.second_norm(self.second_conv(x)))\n",
        "\n",
        "        # Add the input to the output of the second convolution layer\n",
        "        # This is the skip connection\n",
        "        x = x + self.shortcut(input)\n",
        "        return x\n",
        "\n",
        "# Implement the DownSample block that occurs after each residual block\n",
        "class down_sample(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the downsample block\n",
        "    def forward(self, x):\n",
        "        x = self.max_pool(x)\n",
        "        return x\n",
        "\n",
        "# Implement the UpSample block that occurs in the decoder path/expanding path\n",
        "class up_sample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution transpose layer to upsample the input\n",
        "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the upsample block\n",
        "    def forward(self, x):\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "# Implement the crop and concatenate layer\n",
        "class crop_and_concatenate(nn.Module):\n",
        "    def forward(self, upsampled, bypass):\n",
        "        # Crop the upsampled feature map to match the dimensions of the bypass feature map\n",
        "        upsampled = torchvision.transforms.functional.resize(upsampled, size = bypass.shape[2:], antialias=True)\n",
        "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
        "        return x\n",
        "\n",
        "# Implement an attention block\n",
        "class attention_block(nn.Module):\n",
        "    def __init__(self, skip_channels, gate_channels, inter_channels = None, n_groups = 8):\n",
        "        super().__init__()\n",
        "\n",
        "        if inter_channels is None:\n",
        "            inter_channels = skip_channels // 2\n",
        "\n",
        "        # Implement W_g i.e the convolution layer that operates on the gate signal\n",
        "        # Upsample gate signal to be the same size as the skip connection\n",
        "        self.W_g = up_sample(in_channels = gate_channels, out_channels = skip_channels)\n",
        "        #self.W_g_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
        "        #self.W_g_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement W_x i.e the convolution layer that operates on the skip connection\n",
        "        self.W_x = nn.Conv2d(in_channels = skip_channels, out_channels = inter_channels, kernel_size = 1)\n",
        "        #self.W_x_norm = nn.GroupNorm(num_groups = n_groups, num_channels = inter_channels)\n",
        "        #self.W_x_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement phi i.e the convolution layer that operates on the output of W_x + W_g\n",
        "        self.phi = nn.Conv2d(in_channels = inter_channels, out_channels = 1, kernel_size = 1)\n",
        "        #self.phi_norm = nn.GroupNorm(num_groups = n_groups, num_channels = 1)\n",
        "        #self.phi_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement the sigmoid activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # Implement the Swish activation function\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Implement final group normalization layer\n",
        "        self.final_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
        "\n",
        "    # Pass the input through the attention block\n",
        "    def forward(self, skip_connection, gate_signal):\n",
        "        # Upsample the gate signal to match the channels of the skip connection\n",
        "        gate_signal = self.W_g(gate_signal)\n",
        "        # Ensure that the sizes of the skip connection and the gate signal match before addition\n",
        "        if gate_signal.shape[2:] != skip_connection.shape[2:]:\n",
        "            gate_signal = torchvision.transforms.functional.resize(gate_signal, size = skip_connection.shape[2:], antialias=True)\n",
        "        # Project to the intermediate channels\n",
        "        gate_signal = self.W_x(gate_signal)\n",
        "\n",
        "        # Project the skip connection to the intermediate channels\n",
        "        skip_signal = self.W_x(skip_connection)\n",
        "\n",
        "        # Add the skip connection and the gate signal\n",
        "        add_xg = gate_signal + skip_signal\n",
        "\n",
        "        # Pass the output of the addition through the activation function\n",
        "        add_xg = self.act(add_xg)\n",
        "\n",
        "        # Pass the output of attention through a 1x1 convolution layer to obtain the attention map\n",
        "        attention_map = self.sigmoid(self.phi(add_xg))\n",
        "\n",
        "        # Multiply the skip connection with the attention map\n",
        "        # Perform element-wise multiplication\n",
        "        skip_connection = torch.mul(skip_connection, attention_map)\n",
        "\n",
        "        skip_connection = nn.Conv2d(in_channels = skip_connection.shape[1], out_channels = skip_connection.shape[1], kernel_size = 1)(skip_connection)\n",
        "        skip_connection = self.act(self.final_norm(skip_connection))\n",
        "\n",
        "        return skip_connection\n",
        "\n",
        "\n",
        "## Implement a residual attention U-Net\n",
        "class ResidualAttentionUnet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_groups = 4, n_channels = [64, 128, 256, 512, 1024]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the contracting path: residual blocks followed by downsampling\n",
        "        self.down_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(in_channels, n_channels[0]), (n_channels[0], n_channels[1]), (n_channels[1], n_channels[2]), (n_channels[2], n_channels[3])])\n",
        "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
        "\n",
        "        # Define the bottleneck residual block\n",
        "        self.bottleneck = residual_block(n_channels[3], n_channels[4])\n",
        "\n",
        "        # Define the attention blocks\n",
        "        self.attention_blocks = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
        "                                              [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "\n",
        "        # Define the expanding path: upsample blocks, followed by crop and concatenate, followed by residual blocks\n",
        "        self.upsamples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "\n",
        "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
        "\n",
        "        self.up_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                     [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "\n",
        "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
        "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
        "        # while leaving the spatial dimensions unchanged.\n",
        "        self.final_conv = nn.Conv2d(in_channels = n_channels[0] , out_channels = out_channels, kernel_size = 1)\n",
        "\n",
        "    # Pass the input through the residual attention U-Net\n",
        "    def forward(self, x):\n",
        "        # Store the skip connections\n",
        "        skip_connections = []\n",
        "        # # Store the gate signals\n",
        "        # gate_signals = []\n",
        "\n",
        "        # Pass the input through the contracting path\n",
        "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
        "            x = down_conv(x)\n",
        "            skip_connections.append(x)\n",
        "            #gate_signals.append(x)\n",
        "            x = down_sample(x)\n",
        "\n",
        "        # Pass the output of the contracting path through the bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections.append(x)\n",
        "\n",
        "        # Attention on the residual connections\n",
        "        #skip_connections = skip_connections[::-1]\n",
        "        n = len(skip_connections)\n",
        "        indices = [(n - 1 - i, n - 2 - i) for i in range(n - 1)]\n",
        "        attentions = []\n",
        "        for i, g_x in enumerate(indices):\n",
        "            g_gate = g_x[0]\n",
        "            x_residual = g_x[1]\n",
        "            attn = self.attention_blocks[i](skip_connections[x_residual], skip_connections[g_gate])\n",
        "            attentions.append(attn)\n",
        "\n",
        "        #attentions = attentions[::-1]\n",
        "\n",
        "        # Pass the output of the attention blocks through the expanding path\n",
        "        for up_sample, concat, up_conv in zip(self.upsamples, self.concat, self.up_conv):\n",
        "            x = up_sample(x)\n",
        "            x = concat(x, attentions.pop(0))\n",
        "            x = up_conv(x)\n",
        "\n",
        "        # Pass the output of the expanding path through the final convolution layer\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "## Sanity check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResidualAttentionUnet(in_channels = 3, out_channels = 1).to(device)\n",
        "x = torch.randn((1, 3, 32, 32)).to(device)\n",
        "mask = model(x)\n",
        "mask.shape\n",
        "\n",
        "# See how data flows through the network\n",
        "torchsummary.summary(model, input_size = (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lls_HclWTqwe"
      },
      "source": [
        "## Training UNet_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6NQx9nYTqgT"
      },
      "source": [
        "## Testing UNet_2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.12.2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
